Параметризация потребляемых docker-контейнером ресурсов

#######################################################
#     Runtime options with Memory, CPUs, and GPUs     #
# (Параметры времени выполнения с памятью, CPU и GPU) #
#######################################################

  По умолчанию контейнер НЕ ИМЕЕТ ОГРАНИЧЕНИЙ по ресурсам
  и может использовать столько ресурсов, сколько позволяет планировщик ядра хоста.

  Docker предоставляет возможность контролировать, сколько памяти или процессора может использовать контейнер,
  устанавливая флаги конфигурации времени выполнения (runtime configuration flags) команды docker run.
  
В этом разделе подробно описано:
  -> когда следует устанавливать такие ограничения   и
  -> каковы возможные последствия их установки


(!) Многие из этих функций требуют, чтобы ваше ядро поддерживало возможности Linux.

(->) Чтобы проверить наличие поддержки, можно воспользоваться командой:
---
$ docker info
---
  Если какая-либо возможность отключена в вашем ядре, вы можете увидеть предупреждение в конце вывода,
(как показано ниже):
---
WARNING: No swap limit support
---

## Для их включения обратитесь к документации по операционной системе.
## Дополнительные сведения см. в руководстве по устранению неполадок Docker Engine:
#-> https://docs.docker.com/engine/install/troubleshoot/#kernel-cgroup-swap-limit-capabilities


##########
# Memory #
##########

**********************************************
* Понимание того, чем грозит нехватка памяти *
**********************************************

[!] Важно не позволять запущенному контейнеру потреблять слишком много памяти хост-машины!

[Механизм Linux для борьбы с нехваткой памяти:]
(На хостах Linux):
  если ядро обнаруживает, что памяти недостаточно для выполнения важных системных функций,
  оно выбрасывает OOME (= Out Of Memory Exception), и начинает убивать процессы, чтобы освободить память.
  
  Убить можно ЛЮБОЙ ПРОЦЕСС, включая Docker и другие важные приложения.
[!] Это может привести к КРАХУ ВСЕЙ СИСТЕМЫ, если будет убит не тот процесс.

Docker пытается снизить эти риски, регулируя приоритет OOM для демона Docker,
чтобы его вероятность быть убитым была ниже, чем у других процессов в системе.

[!] Приоритет OOM для контейнеров НЕ КОРРЕКТИРУЕТСЯ.
    Таким образом, вероятность УНИЧТОЖЕНИЯ ОТДЕЛЬНОГО КОНТЕЙНЕРА ВЫШЕ,
    чем вероятность уничтожения демона Docker или других системных процессов.

[!] НЕ СТОИТ пытаться обойти эти меры защиты, 
    вручную устанавливая --oom-score-adj в экстремально отрицательное число для демона или контейнера,
    или устанавливая --oom-kill-disable для контейнера.

Дополнительную информацию об управлении OOM в ядре Linux см. в разделе Out of Memory Management:
(https://www.kernel.org/doc/gorman/html/understand/understand016.html)

********************************************************************
* Мероприятия по снижению риска нестабильности системы из-за OOME: *
********************************************************************

  ➜ Проведите тесты, чтобы понять требования к памяти вашего приложения, прежде чем запускать его в production.
  ➜ Убедитесь, что ваше приложение запускается только на хостах с достаточными ресурсами.

  ➜ Ограничьте объем памяти, который может использовать ваш контейнер, (как описано ниже).

  ➜ Будьте внимательны при настройке свопа на хостах Docker.
     Подкачка (swap) медленнее, чем Memory, но может служить БУФЕРОМ на случай нехватки системной памяти.

  ➜ Рассмотрите возможность преобразования контейнера в сервис (https://docs.docker.com/engine/swarm/services/)
     и использования ограничений на уровне сервиса и меток нод (nodes)
     для обеспечения запуска приложения только на нодах С ДОСТАТОЧНЫМ КОЛИЧЕСТВОМ ПАМЯТИ.


*****************************************
* Ограничьте доступ контейнера к памяти *
*****************************************

  Docker может накладывать ЖЕСТКИЕ или МЯГКИЕ ограничения на память.

  Жесткие ограничения
    * позволяют контейнеру использовать НЕ БОЛЕЕ фиксированного объема памяти.

  Мягкие ограничения
    * позволяют контейнеру использовать столько памяти, сколько ему нужно, если не выполняются определенные условия,
       (например, когда ядро обнаруживает нехватку памяти или нехватку ресурсов на хост-машине).

# Некоторые из этих опций имеют различные эффекты при использовании по отдельности или при установке нескольких опций.

Большинство из этих опций принимают целое положительное число, за которым следует суффикс:
  * b = байты,
  * k = килобайты,
  * m = мегабайты,
  * g = гигабайты


[Опции:]

  -m, --memory=
    -> Максимальный объем памяти, который может использовать контейнер.
       Если вы установите этот параметр, минимально допустимое значение будет 6m (6 мегабайт).
       То есть вы должны установить значение НЕ МЕНЕЕ 6 мегабайт.

  --memory-swap
    -> Объем памяти, который этот контейнер может свопировать на диск.
       Подробнее см. в разделе: "--memory-swap details"
       (https://docs.docker.com/config/containers/resource_constraints/#--memory-swap-details)

  --memory-swappiness
    -> По умолчанию ядро хоста может менять местами определенный процент анонимных страниц, используемых контейнером.
       Вы можете установить значение --memory-swappiness в диапазоне от 0 до 100, чтобы настроить этот процент.
       Подробности см. в разделе "--memory-swappiness":
       (https://docs.docker.com/config/containers/resource_constraints/#--memory-swappiness-details)

  --memory-reservation
    -> Позволяет указать мягкий лимит, меньший, чем --memory,
       который активируется, когда Docker обнаруживает нехватку или недостаток памяти на хост-машине.
       Если вы используете параметр --memory-reservation, он должен быть установлен НИЖЕ, чем --memory,
       чтобы иметь приоритет.
       Поскольку это МЯГКОЕ ограничение, оно НЕ ГАРАНТИРУЕТ, что контейнер не превысит лимит.

  --kernel-memory 
    -> Максимальный объем памяти ядра, который может использовать контейнер.
       Минимально допустимое значение - 6m.
       Поскольку память ядра не может быть вытеснена (swapped out),
       контейнер, которому не хватает памяти ядра, может блокировать ресурсы хост-машины,
       что может иметь побочные эффекты для хост-машины и для других контейнеров.
       Подробности см. в параметре "--kernel-memory details":
       (https://docs.docker.com/config/containers/resource_constraints/#--kernel-memory-details)

  --oom-kill-disable 
    -> По умолчанию при возникновении ошибки out-of-memory (OOM) ядро убивает процессы в контейнере.
       Чтобы изменить это поведение, используйте параметр --oom-kill-disable.
       Отключать OOM killer можно ТОЛЬКО в тех контейнерах, где также установлен флаг -m/--memory.
  [!]  Если флаг -m НЕ УСТАНОВЛЕН, на хосте может закончиться память,
       и ядру может потребоваться убить процессы хост-системы, чтобы освободить память.

## Дополнительные сведения о cgroups и памяти в целом см. в документации по Memory Resource Controller:
# (https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt)


************************
* Детали --memory-swap *
************************
  --memory-swap - это флаг-модификатор, который имеет значение только в том случае, если также установлен флаг --memory.
  Использование свопа позволяет контейнеру записывать на диск избыточные потребности в памяти,
  когда контейнер исчерпал всю доступную ему оперативную память.

[!]  Приложения, часто использующие подмену памяти на диск, теряют в производительности.

(*) Его настройка может иметь сложные последствия:
  -> Если параметр --memory-swap имеет целое положительное значение,
     => то должны быть установлены оба параметра --memory и --memory-swap.
     --memory-swap представляет собой ОБЩИЙ ОБЪЕМ ПАМЯТИ И СВОПА,
     который может быть использован,
     а --memory контролирует объем, используемый не свопируемой памятью.
-->
#[Пример:]
#  если --memory="300m" и --memory-swap="1g",
#  контейнер может использовать 300m памяти и 700m (1g - 300m) swap.
<--

  -> Если параметр --memory-swap установлен в 0,
     => то настройка игнорируется, и значение рассматривается как несброшенное.

  -> Если --memory-swap имеет ТО ЖЕ ЗНАЧЕНИЕ, что и --memory,
     а --memory имеет положительное целое число,\
     => контейнер не будет иметь доступа к swap.
     (См. "Запретить контейнеру использовать своп")

  -> Если параметр --memory-swap не установлен, а --memory установлен,
     => контейнер может использовать столько свопа, сколько установлено в параметре --memory,
     если в хост-контейнере настроена своп-память.
-->
#[Например:]
#  если --memory="300m" и --memory-swap не задано,
#  == контейнер может использовать 600m в сумме (памяти + swap)

  -> Если параметр --memory-swap явно установлен в -1,
     => контейнеру разрешается использовать неограниченное количество swap,
        вплоть до объема, доступного на хост-системе.

[! Внутри контейнера:]
  такие инструменты, как free, сообщают О ДОСТУПНОМ СВОПЕ НА ХОСТЕ,
  а НЕ О ТОМ, ЧТО ДОСТУПНО ВНУТРИ КОНТЕЙНЕРА!

[!]  НЕ ПОЛАГАЙТЕСЬ на вывод free или аналогичных инструментов для определения наличия свопа.

[Запретите контейнеру использовать своп:]
  Если для --memory и --memory-swap установлено одинаковое значение, это не позволит контейнерам использовать своп.
  Это происходит потому, что
    --memory-swap - это количество комбинированной памяти и свопа, которое может быть использовано,
    --memory - это только количество физической памяти, которое может быть использовано.

<-- Управление лимитами memory в Docker Compose -->
+(https://for-each.dev/lessons/b/-ops-docker-memory-limit)

#######
# CPU #
#######
(https://docs.docker.com/config/containers/resource_constraints/#cpu)

  По умолчанию доступ каждого контейнера к процессорным циклам хост-машины НЕОГРАНИЧЕН.
  Можно установить различные ограничения, чтобы ограничить доступ данного контейнера к циклам ЦП хост-машины.
  Большинство пользователей используют и настраивают планировщик CFS (CFS scheduler) по умолчанию.
# (https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler)

  Можно также настроить планировщик реального времени (real-time-scheduler).
# (https://docs.docker.com/config/containers/resource_constraints/#configure-the-real-time-scheduler)


***************************************
* Configure the default CFS scheduler *
***************************************
[source: https://docs.docker.com/config/containers/resource_constraints/#configure-the-real-time-scheduler]

  CFS - это планировщик CPU ядра Linux для обычных процессов Linux.
  Несколько флагов времени выполнения позволяют настроить объем доступа к ресурсам ЦП, который имеет контейнер.
[!]  Когда вы используете эти параметры, Docker изменяет настройки cgroup контейнера на хост-машине.

  --cpus=<value>
    = укажите, какую часть доступных ресурсов центрального процессора может использовать контейнер.
    Например:
      если хост-машина имеет 2 процессора
      и вы задали --cpus="1.5",
      => контейнеру будет гарантировано НЕ БОЛЕЕ ПОЛУТОРА ПРОЦЕССОРОВ.
    (!) Это эквивалентно установке
      --cpu-period="100000"
        <и>
      --cpu-quota="150000"

  --cpu-period=<value>
    = укажите период планировщика CPU CFS, который используется вместе с --cpu-quota.
    (По умолчанию - 100000 микросекунд == 100 миллисекунд).
    (*) Большинство пользователей не меняют это значение по умолчанию.
    (!) Для большинства случаев использования --cpus является более удобной альтернативой.

  --cpu-quota=<value>
    = Наложение квоты CPU CFS на контейнер.
    Количество микросекунд за --cpu-период, на которое ограничен контейнер перед дросселированием.
    В этом случае он выступает в качестве эффективного потолка.
    (!) Для большинства случаев использования --cpus является более удобной альтернативой.

  --cpuset-cpus
    = Ограничение конкретных процессоров или ядер, которые может использовать контейнер.
    Список, разделенный запятыми,
    или диапазон процессоров, разделенных дефисом,
      которые может использовать контейнер, если у вас более одного процессора.
      Первый процессор имеет номер 0.
      Правильное значение может быть:
        * 0-3 (для использования первого, второго, третьего и четвертого процессора)
        <или>
        * 1,3 (для использования второго и четвертого процессора).

  --cpu-shares
    = Установите этот флаг в значение, большее или меньшее, чем 1024 по умолчанию,
    чтобы увеличить или уменьшить вес контейнера и предоставить ему доступ к большей или меньшей доле циклов ЦП главной машины.
    (!) Это выполняется только в случае нехватки циклов ЦП.
    При достаточном количестве циклов ЦП все контейнеры используют столько ЦП, сколько им нужно.
    Таким образом, это МЯГКОЕ ОГРАНИЧЕНИЕ.
  (!) --cpu-shares не препятствует планированию контейнеров в режиме Swarm.
      Она определяет приоритет ресурсов ЦП контейнеров для доступных циклов ЦП.
    (!) Он НЕ ГАРАНТИРУЕТ и НЕ РЕЗЕРВИРУЕТ какой-либо конкретный доступ к процессору.


  Если у вас 1 процессор, каждая из следующих команд гарантирует контейнеру не более 50 % процессора каждую секунду.
$ docker run -it --cpus=".5" ubuntu /bin/bash

# Это эквивалентно ручному указанию --cpu-period и --cpu-quota;
$ docker run -it --cpu-period=100000 --cpu-quota=50000 ubuntu /bin/bash

################################################
# CPU & memory settings in docker compose file #
#         Compose Deploy Specification         #
################################################
(https://docs.docker.com/compose/compose-file/deploy/)

Deploy - это необязательная часть спецификации Compose.
Она используется для настройки развертывания и управления сервисами в режиме Docker Swarm.
По сути, она предоставляет набор спецификаций развертывания для управления поведением контейнеров в различных средах.

********************
* deploy.resources *
********************
(https://docs.docker.com/compose/compose-file/deploy/#resources)

  resources:
    = настраивает ограничения физических ресурсов для запуска контейнера на платформе.
      Эти ограничения могут быть настроены как:

      limits:
        = платформа НЕ ДОЛЖНА ПОЗВОЛЯТЬ контейнеру выделять больше.

      reservations:
        = платформа должна ГАРАНТИРОВАТЬ, что контейнер сможет выделить не менее настроенного объема.

---[Пример:]---
services:
  frontend:
    image: example/webapp
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 50M
          pids: 1
        reservations:
          cpus: '0.25'
          memory: 20M
---

● cpus
    = настраивает ограничение или резервирование на то, сколько доступных ресурсов процессора, в виде количества ядер, может использовать контейнер

● memory
    = настраивает ограничение или резервирование объема памяти, который может выделить контейнер;
    задается в виде строки, выражающей значение байта:
    (https://docs.docker.com/compose/compose-file/11-extension/#specifying-byte-values)

● pids
    = 

● devices
    = настраивает резервирование устройств, которые может использовать контейнер.
      Он содержит список резервирований, каждое из которых задается как объект со следующими параметрами:
      ➜ capabilities,
      ➜ driver,
      ➜ count,
      ➜ device_ids
      ➜ options.


mem_limit
(https://docs.docker.com/compose/compose-file/05-services/#mem_limit)



memswap_limit

  memswap_limit определяет объем памяти, который контейнеру разрешено свопировать на диск.
  Это модифицирующий атрибут, который имеет значение только в том случае, если также ЗАДАН ПАРАМЕТР memory.
  Использование подкачки позволяет контейнеру записывать на диск избыточные объемы памяти,
  когда контейнер исчерпал всю доступную ему память.
  Приложения, ЧАСТО использующие подкачку памяти на диск, ТЕРЯЮТ В ПРОИЗВОДИТЕЛЬНОСТИ.

  ➜ Если memswap_limit задан положительным целым числом
       то должны быть заданы и memory, и memswap_limit.
       memswap_limit представляет собой общий объем памяти и свопа, который может быть использован,
       а memory контролирует объем, используемый не свопируемой памятью.
       Так, если
         memory="300m"
           и
         memswap_limit="1g",
       контейнер может использовать 300m памяти и 700m (1g - 300m) swap.

  ➜ Если memswap_limit установлен в 0
       то настройка игнорируется, и значение рассматривается как неустановленное.

  ➜ Если memswap_limit установлен в то же значение, что и память, а память имеет положительное целое число
       контейнер НЕ ИМЕЕТ ДОСТУПА к свопу.

  ➜ Если memswap_limit не установлен, а память установлена
      контейнер может использовать столько свопа, сколько установлено в памяти,
      если в хост-контейнере настроена своп-память.
      Например, если
        memory="300m"
          и
        memswap_limit не установлен,
      контейнер может использовать 600m в сумме памяти и swap.

  ➜ Если memswap_limit явно установлен в -1
       контейнеру разрешается использовать НЕОГРАНИЧЕННЫЙ СВОП,
       вплоть до объема, доступного на хост-системе.


******************
* restart_policy *
******************
(https://docs.docker.com/compose/compose-file/deploy/#restart_policy)


###########################################
# Setting Memory And CPU Limits In Docker #
###########################################

(https://www.baeldung.com/ops/docker-memory-limit)

Существует множество случаев, когда нам необходимо ограничить использование ресурсов на хост-машине docker.
Рассмотрим, как установить лимит памяти (memory) и процессора (CPU) для контейнеров docker.

**************************************************
* Установка лимита ресурсов с помощью docker run *
**************************************************

  "+"  простое решение
  "-"  ограничение будет применяться только к одному конкретному исполнению образа.

[memory:]

  Например, ограничим память, которую может использовать контейнер, 512 мегабайтами.
  Чтобы ограничить память, нам нужно использовать параметр m:
---
$ docker run -m 512m nginx
---

  Мы также можем установить МЯГКИЙ ЛИМИТ, называемый reservation (резервированием).
  Он активируется, когда docker обнаруживает недостаток памяти на хост-машине:
---
$ docker run -m 512m --memory-reservation=256m nginx
---


[CPU:]

  ПО УМОЛЧАНИЮ доступ к вычислительной мощности хост-машины НЕОГРАНИЧЕН.
  Мы можем установить ограничение на КОЛИЧЕСТВО ПРОЦЕССОРОВ с помощью параметра cpus.

  Пример:
    -> ограничим наш контейнер, чтобы он использовал не более двух процессоров:
---
$ docker run --cpus=2 nginx
---

  Мы также можем указать приоритет выделения процессора.
  По умолчанию это 1024, более большие числа имеют более высокий приоритет:

---
$ docker run --cpus=2 --cpu-shares=2000 nginx
---
  Как и резервирование памяти, доля процессора играет главную роль,
  когда вычислительная мощность ограничена и должна быть разделена между конкурирующими процессами.


**********************************************************
* Установка лимита памяти с помощью файла docker-compose *
**********************************************************

  Мы можем добиться аналогичных результатов, используя файлы docker-compose.
[!] Помните, что формат и возможности в разных версиях docker-compose МОГУТ ОТЛИЧАТЬСЯ.

[!] Нам нужно создать сегменты deploy и resources в конфигурации службы:

---<docker-compose.yml>---
services:
  service:
    image: nginx
    deploy:
        resources:
            limits:
              cpus: 0.50
              memory: 512M
            reservations:
              cpus: 0.25
              memory: 128M
---

[!] Чтобы воспользоваться сегментом deploy в файле docker-compose,
    нам нужно использовать команду
-->
docker stack.
<--

Чтобы развернуть стек в swarm, мы выполняем команду deploy:
---
$ docker stack deploy --compose-file docker-compose.yml bael_stack
---


*************************************
* Версия 2 С помощью docker-compose *
*************************************

  В старых версиях docker-compose мы могли устанавливать ограничения на ресурсы на том же уровне, что и основные свойства сервиса.
  Они также имеют несколько иное название:

---<docker-compose.yaml>---
service:
  image: nginx
  mem_limit: 512m
  mem_reservation: 128M
  cpus: 0.5
  ports:
    - "80:80"
---

  Чтобы создать сконфигурированные контейнеры, нам нужно выполнить команду docker-compose:
---
$ docker-compose up
---


***********************************
* Проверка использования ресурсов *
*    Verifying Resources Usage    *
***********************************

[!] После того как мы установили ограничения, мы можем проверить их с помощью команды docker stats:
---
$ docker stats
CONTAINER ID        NAME                                             CPU %     MEM USAGE / LIMIT   MEM %       NET I/O         BLOCK I/O           PIDS
8ad2f2c17078        bael_stack_service.1.jz2ks49finy61kiq1r12da73k   0.00%     2.578MiB / 512MiB   0.50%       936B / 0B       0B / 0B             2
---