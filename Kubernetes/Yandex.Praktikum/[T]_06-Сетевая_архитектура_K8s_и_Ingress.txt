Сетевая архитектура Kubernetes и Ingress


[Из этого урока вы:]
  ✓ Узнаете о сетевой архитектуре Kubernetes для приложений.
  ✓ Узнаете, что такое CNI и какие бывают плагины, поддерживающие эту спецификацию.
  ✓ Узнаете, что такое Ingress Controller, и запустите Ingress для приложения.


Кластер Kubernetes представляет собой готовую инфраструктуру для запуска приложений.
Оркестратор обеспечивает необходимую сетевую связность
  + для общения приложений в Pod'ах между собой 
  + и для внешнего доступа пользователей к приложениям.


#####################
# Сетевая топология #
#####################

****************
* Node Network *
****************

В рамках Node Network происходит общение
  [[Worker-Nodes]] <--> [Control Plane кластера]
  [[Worker-Nodes]] <--> [[Worker-Nodes]]  # между собой

  + работа системного администратора с серверами.


***************
* Pod Network *
***************

  При развёртывании кластера и для назначения IP-адресов Pod'ам приложений
  администратор определяет большую ПОДСЕТЬ, НЕ СВЯЗАННУЮ с Node Network.
  
  Из этой большой подсети на каждую Worker-ноду выделяется подсеть меньшего размера,
  дабы каждая Worker-нода имела своё выделенное адресное пространство для Pod'ов.
  
                                                                     ┌─[Worker_0](10.1.101.0/24)->[Pod_1](10.1.101.3)
[Control Plane]<-->{Pod Network (10.1.0.0./16)}<-->{Node Network}<-->├─[Worker_1](10.1.102.0/24)->[Pod_1](10.1.102.2)
                                                                     └─[Worker_N](10.1.1XX.0/24)->[Pod_1](10.1.1XX.Y)


***************
* CNI-плагины *
***************

  Каждый Pod получает свой собственный выделенный IP-адрес;

  Архитектура Kubernetes определяет следующие требования к сетевой связности Pod'ов:
    ✓ Pod'ы на определённой Worker-ноде умели взаимодействовать по сети
      + со ВСЕМИ Pod'ами НА ЛЮБЫХ Worker-нодах
      + напрямую и без NAT;

    ✓ Сервисы на Worker-ноде (такие как kubelet) умели взаимодействовать с Pod'ами на этой Worker-ноде
      + так же напрямую и без NAT

  Обеспечение этих требований ложится на плечи плагинов CNI
    = отдельно устанавливаемых компонентов кластера,
      управляющих правилами маршрутизации сетевого траффика
      и занимающихся созданием сетевых интерфейсов для подов.

CNI или «Container Network Interface»
  — это спецификация правил сетевого взаимодействия контейнеров,
    а также правил создания и удаления сетевых интерфейсов для контейнеров.

[Замечание:]
"""
  Плагинов, удовлетворяющих спецификации CNI, развелось очень много,
  и каждый из них может решать дополнительные задачи, такие как
    ✓ создание оверлейных (overlay) сетей для взаимодействия Pod'ов
  <или>
    ✓ обмен маршрутной информацией (таблицы маршрутизации/топология)
      с помощью протоколов динамической маршрутизации.
"""

[Рассмотрим простейший случай:]
  * на Worker-нодах Linux создаётся виртуальный коммутатор (linux bridge), к которому подключаются Pod'ы;
  * в локальную таблицу маршрутизации добавляется маршрут до подсетей Pod'ов на других Worker-нодах.

  Так работает CNI-плагин kubenet =
  = встроенный в kubelet плагин с самой базовой функциональностью.


< Задание 1: Какие задачи могут решать CNI-плагины? >
  ✓ Организация сети в кроссплатформенных инсталляциях Kubernetes
  ✓ Масштабирование кластера на несколько ЦОДов
  ✓ Интеграция с облачными провайдерами
  ✓ Реализация сетевых политик доступа к Pod'ам


*******************
* Service Network *
*******************

  Сущность Service в Kubernetes представляет собой именованную абстракцию для доступа к НАБОРУ Pod'ов.

  Набор Pod'ов определяется с помощью списка лейблов (labels)
  — Pod'ы, которые имеют указанные в селекторе labels, будут доступны через этот сервис.

Pod'ы могут динамически создаваться и удаляться с произвольными адресами => 
  => в K8s добавлена сущность endpoints:
      * работающая в паре с каждым сервисом
      * и обновляемая динамически.

{ По мере создания и удаления Pod'ов
  в списке endpoints обновляются IP-адреса этих Pod'ов:
---
> kubectl get svc/kube-dns -n kube-system
####
NAME       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   172.20.0.10   <none>        53/UDP,53/TCP   67d

> kubectl get endpoints/kube-dns -n kube-system
####
NAME       ENDPOINTS                                                   AGE
kube-dns   10.1.101.179:53,10.1.101.8:53,10.1.101.179:53 + 1 more...   67d
---
}

# TYPE сервиса определяет способ доставки трафика до Pod'ов.
# By default: ClusterIP

  Адрес такого сервиса выделяется из диапазона [Service Network]
  = подсети, УКАЗАННОЙ ПРИ УСТАНОВКЕ Kubernetes КЛАСТЕРА администратором.


[На Worker-Node:]
  сервис реализован с помощью технологий балансировки iptables или IP Virtual Server (IPVS)
  — встроенного в ядро Linux балансировщика для протоколов TCP и UDP.

"""
  IPVS (IP Virtual Server) реализует балансировку нагрузки на транспортном уровне внутри ядра Linux,
  так называемую коммутацию на уровне 4.
  IPVS, запущенный на хосте, действует как балансировщик нагрузки перед кластером реальных серверов,
  он может направлять запросы на сервисы, основанные на TCP/UDP, реальным серверам,
  и заставляет сервисы реальных серверов отображаться как виртуальные сервисы на одном IP-адресе.
#-> http://www.linuxvirtualserver.org/software/ipvs.html
"""

  Iptables
    + ещё используется в большинстве инсталляций,
    - имеет ограниченные возможности масштабирования
      (обилие правил может замедлить сетевое взаимодействие)

[kube-proxy]
  Компонент kube-proxy на каждой Worker-ноде
    ✓ отслеживает события создания Service'ов
    ✓ и добавляет соответствующие iptables-правила в NAT-таблицу
      для трансляции адреса Service'а на адреса Pod'ов
      или создаёт IPVS балансировщик адреса Service'а на адреса Pod'ов.

Типы:
  > NodePort
  > LoadBalancer
  решают задачу доступа внешних пользователей к приложениям внутри кластера Kubernetes.


************
* NodePort *
************
  позволяет подключиться к приложению через IP-адрес Worker-ноды.
  При запуске кластера Kubernetes определяется диапазон портов
  (30000-32767 по умолчанию),
  из которого выделяется очередной номер порта для нового сервиса.
  В этом случае можно получить доступ к приложению, подключившись к
--[Доступ к приложению:]--
<адресу Worker-ноды>:<номер выделенного порта>.
--

[Под капотом сервиса типа NodePort:]
  -> автоматически создаются правила балансировки на ClusterIP, за которые отвечает kube-proxy на каждой Worker-ноде.


****************
* LoadBalancer *
****************
  создаётся с помощью ВНЕШНЕГО балансировщика.
  При использовании ОБЛАЧНЫХ ПРОВАЙДЕРОВ такой балансировщик создаётся АВТОМАТИЧЕСКИ при запросе создания Service'а.

[Под капотом:]
  -> автоматически создаётся сервис NodePort и соответствующий ему ClusterIP.

  Внешний балансировщик выполняет балансировку по адресам:
--[Балансировка по адресам]--
<адрес Worker-ноды>:<номер выделенного порта NodePort>
--


<Вопрос:  почему сервис типа NodePort НЕ СТОИТ ИСПОЛЬЗОВАТЬ для внешнего доступа к приложению в Kubernetes?>
[ОТВЕТ:]
  1) При работе нужно знать адрес Worker-ноды, к которому не всегда есть доступ;
  2) Нужно заботиться о переключении трафика на другую Worker-ноду, когда рабочая выходит из строя.

  + Сервис типа NodePort имеет специальное назначение
    и чаще всего работает в связке с сервисом типа LoadBalancer.



###########
# Ingress #
###########
(https://kubernetes.io/docs/concepts/services-networking/ingress/)

  - Сетевые балансировщики у облачных провайдеров стоят ДОРОГО
  - Сетевые балансировщики предоставляют простые правила доступа на уровне протоколов TCP || UDP.

  * Для приложений часто требуется:
    + поддержка TLS (HTTPS),
    + маршрутизация http-запросов на основе доменных имён или путей в URL,

  => поэтому в Kubernetes этим занимается специальный API-ресурс Ingress.

[Основное назначение Ingress:]
  = обеспечение внешнего доступа к HTTP/HTTPS-приложениям
    # в то время как другой тип трафика лучше прогонять через сервисы типа
    # NodePort или LoadBalancer.

Сообщество Kubernetes оставило вопрос обработки запроса на создание Ingress открытым.
#(Главная цель: гибкость интеграции кластеров с инфраструктурами различного типа).

[Администратор]-->{устанавливает (как обычное приложение Kubernetes)}-->(контроллер Ingress Controller)-->[КЛАСТЕР]

В большинстве инсталляций используется популярный NGINX Ingress Controller.
/*
#[sources:]
#  1) "NGINX Ingress Controller": https://docs.nginx.com/nginx-ingress-controller/
#  2) "Installation Guide": https://kubernetes.github.io/ingress-nginx/deploy/
*/

NGINX Ingress Controller:
  + запускает сам контроллер,
  + Pod'ы с Nginx
  + создаёт сервис типа LoadBalancer для этих Pod'ов и протоколов HTTP и HTTPS.

  Контроллер:
    ✓ отслеживает события запросов на создание || изменение Ingress-ресурсов
    ✓ добавляет конфигурацию в Nginx для маршрутизации траффика к приложению.

# На примере типового Ingress-ресурса рассмотрим логику публикации приложения с помощью Ingress:
-->
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
spec:
  # В кластере может быть несколько Ingress Controllers,
  # мы используем NGINX
  ingressClassName: "nginx"
  rules:
    # Хост определяет правило направления траффика по доменному имени
    - host: "www.example.com"
      http:
        # Для различных путей в URL можно указать различные бэкенд-сервисы
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
              # Заранее создан сервис типа ClusterIP
              # Он выступает в качестве бэкенда нашего Ingress
                name: frontend
                port:
                # У сервиса может быть несколько портов, указываем нужный нам
                  number: 8080
<--

# Здесь же можно добавить конфигурацию TLS, определив
  ➜ секрет с сертификатом и закрытым ключом,
  ➜ а также доменные имена для этого сертификата:

-->
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
spec:
  ingressClassName: "nginx"
  tls:
    - secretName: ingress-example-tls
      hosts:
        - "www.example.com"
  rules:
  - host: "www.example.com"
    http:
      paths:
      - backend: "<...>"
<--

<Вопрос: какие из этих технологий можно использовать в Ingress Contoller?>

  ✓ Nginx
  ✓ HAproxy
  - iptables
  ✓ Traefik
  ✓ Application Load Balancer в AWS


# На этом этапе уже есть набор манифестов для полноценного деплоя сосисочной в Kubernetes.
# Ingress в их составе открывает доступ внешних пользователей.

[Полезные материалы]

1) Kubernetes Network Model:
  |➜ https://kubernetes.io/docs/concepts/services-networking/

2) Kubernetes Cluster Networking.
  |➜ https://kubernetes.io/docs/concepts/cluster-administration/networking/

3) Kubernetes Network Plugins.
  |➜ https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/

4) Using a Service to Expose Your App
  |➜ https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/

5) IPVS-Based In-Cluster Load Balancing Deep Dive
  |➜ https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/

6) How NGINX Ingress Controller Works.
  |➜ https://docs.nginx.com/nginx-ingress-controller/overview/design/

7) Traefik.
  |➜ https://doc.traefik.io/traefik/v1.7/