Алертинг


  Из этого урока вы узнаете:
  Для чего нужны оповещения (алерты)
  Когда и куда отправлять алерты
  Как выбрать метрики, для которых нужно настраивать алерты
  Как настроить алерты в Grafana
  Как настроить Prometheus Alertmanager



Alerting (с англ. Оповещение) — не более чем отправка сообщения о состоянии системы.

События отличаются различными уровнями важности, по которым легче определить
  - состояние системы,
  - скорость реагирования
  - и необходимость оповещения:
    ● Info
      — что-то произошло, но обычное поведение системы сохраняется, поэтому нет повода для реагирования;

    ● Warning
      — происходит что-то, не являющееся нормальным поведением системы, но при этом и НЕ ЯВЛЯЮЩЕЕСЯ ОШИБКОЙ;

    ● Error
      — явная ошибка в системе, когда что-то сломалось и не смогло отработать;

    ● Critical
      — вся система или часть её перестала работать.


# Примеры алертов:

Info:
  - "На сайте зарегистрировалась 1000 пользователей"
  - "За день на сайт поступило 30 тысяч обращений"

Warning:
  - "Свободного места осталось 10%"
  - "Среднее время отклика сайта выросло до 10 секунд"

Critical:
  - "Количество свободных подключений к PostgreSQL достигло 0"
  - "Сервис Nginx не работает 3 минуты"


(?) Куда можно отправлять сообщения?
(!) Туда, где человек сможет их прочесть:
      ✓ В командный чат: Telegram, Slack, Teams;
      ✓ SaaS-сервисы уведомлений (VictorOps, OpsGenie, PagerDuty, Amixr);
      ✓ SMS;
      ✓ E-Mail.

# ! Зависит от степени критичности и от количества времени, в течение которого должно быть принято решение/выполнено действие
# по урегулированию возникшей ситуации


Механизм реагирования на алерты в каждой команде или компании реализован по-своему,
но главное
  -> чтобы он был в принципе,
  -> и все его чётко понимали.

Обычно в регламенте прописаны:
  ● график дежурства;
  ● инструкции по тому, как реагировать на алерты;
  ● механизм эскалации (1-я, 2-я, 3-я линии поддержки);
  ● Postmortem
      — процесс в котором разбирают инцидент, как он произошел и как его не допустить вновь.
      Включает в себя blameless-культуру.

(*) blameless-культура 
      - культура анализа инцидентов, опирающаяся на исследование и исправление причин инцидента
        БЕЗ ОБВИНЕНИЯ конкретных инженеров и команд в отсутствии компетенций или неправомочных действиях.


Мероприятия, помогающие облегчить жизнь дежурным:
  ➜ Уменьшить количество «ложных» алертов;

  ➜ Уменьшить число пожаров
      (сознательно вложить часть времени команды в увеличение отказоустойчивости инфраструктуры);

  ➜ Делать ротацию дежурных (подневную, недельную или месячную);

  ➜ Если команда международная, то использовать разницу между временными зонами;

  ➜ Иметь резервного дежурного;

  ➜ Обеспечить достойную компенсацию.


******************************************
* Повторяем метрики и немного статистики *
******************************************

  ● Метрика — это показатель, который описывает свойство наблюдаемой системы в определённый момент времени;

  ● Временной ряд — набор значений метрики за период времени.

  ● Метрики можно сгруппировать по типам:
    ◆ Gauge (шкала)
        — численное значение, которое может непредсказуемо увеличиваться и уменьшаться
          (использование CPU, memory, disk; количество пользователей онлайн)
    ◆ Counter (счётчик)
        — численное значение, которое может лишь РАСТИ СО ВРЕМЕНЕМ;
          возможен СБРОС НА НОЛЬ (uptime, количество HTTP запросов, количество регистраций пользователей, число продаж).
    ◆ Histogram/Summary (гистограмма)
        — разбиение значений метрики на диапазоны для более точной оценки происходящего
          (время ответа веб-сервера, размер тела запроса).


/*
Интуитивно понятный пример, знакомый каждому админу:
  Free HDD < 20% — warning
    = свободное место на таком-то сервере подходит к концу, уже меньше 20% —
      (!) примите меры.
  Free HDD < 10% — error
    = свободного места почти не осталось, всего 10% —
      (!!) удалите что-нибудь, пока не стало поздно.
  Free HDD = 0% — critical
    = свободное место на проде кончилось, ничего не работает,
      начальник уже бежит в твою сторону,
      а ваш рейтинг в Google Play и App Store стремится к 1 звезде
*/


<?> как быть, если приложение находится в рабочем состоянии лишь частично?
    Когда лишь часть запросов возвращает ошибку?
    Как тогда строить статистику и определять момент для привлечения внимания дежурного к проблеме?
</?>


СТАТИСТИЧЕСКИЕ МЕТОДЫ:
  + Среднее значение;
  + Медиана;
  + Повторяемость;
  + Стандартное отклонение;
  + Перцентили, процентили, квантили.


Для вычисления многих значений нам понадобится некая агрегация:
  ✓ суммирование,
  ✓ минимальное или максимальное значение,
  ✓ а также перцентили.

Большинство ПРИЛОЖЕНИЙ ДЛЯ СБОРА МЕТРИК И МОНИТОРИНГА имеют такую возможность (в том числе и наш Prometheus).

Среднее арифметическое значение: сложить все времена отклика и поделить их на миллион запросов.


(?) Какие проблемы могут быть в метрике среднего времени отклика сайта?

  -> Запросы с очень большим временем отклика тянут метрику сильно вверх

  -> Запросы с очень маленьким временем ответа тянут статистику сильно вниз

  -> Сильное влияние на статистику любых крайних значений

Экстремумы!
  Они сильно подпортят статистику, исказив реальную картину ответа приложения для подавляющего большинства пользователей.

 
(?) На сервере есть значения load average
3, 3.2, 2.9, 3.5, 4, 3.1, 3.9, 3.9, 3.1, 4
  Какое значение будет соответствовать 80% процентилю?
#! Отсортируйте значения по возрастанию, отделите 80% элементов из такого набора и возьмите крайнее правое значение.

Ответ: 3.9


Как разные приложения по мониторингу данных решают вопросы отправки оповещений:

  - в каких-то сервисах есть встроенный механизм
    #(например, в Zabbix и Grafana),
  - в каких-то отдают на «аутсорс» и вызываются отдельно (например, в Prometheus).

необходимый набор инструментов:
  триггер срабатывания,
  вариативность оповещений для различных окружений,
  формат сообщения,
  выбор мессенджера
  и др.


Рассмотрим две системы для алертов:
  Grafana Alerting vs интеграция Prometheus с Alertmanager

+ Grafana Alerting
    = позволяет настраивать правила нотификаций прямо в Web-интерфейсе Grafana,
      что довольно удобно для задач, связанных НЕ ТОЛЬКО с Kubernetes.

+ Алерты Prometheus раскрывают себя там, где много динамики,
  # и, например, популярный стек мониторинга'*' самого Kubernetes использует Alertmanager.
  (*) https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack


####################
# Grafana Alerting #
####################

В Grafana есть встроенная алерт-система:
  # (в левом меню нажать) -> вкладка Alerting

[Вкладка Alert rules:]
  -> отображены все существующие на текущий момент алерты,
  которые можно всячески фильтровать по дашбордам и их статусам, например:
  * Firing — горящих и требующих внимания;
  * Normal — в состоянии покоя;
  * Pending — переходящих из статуса Normal в Firing.

[Вкладка Contact points:]
  -> можно настроить способы отправки сообщений:
     от Email, Google Hangout, Teams
     до Telegram, Slack
     и даже кастомных webhook'ов.


# < Конфигурация для нашего Telegram >

# Для этого достаточно дать имя новому "contact point",
# выбрать тип Telegram и указать BOT API Token и Chat ID.

[Другие вкладки в Alerting]
  -> тоже интересные: можно
    + группировать алерты,
    + настраивать рабочее время, в которое будут приходить оповещения,
    + а когда можно и поспать или просто сохранить все настройки как json-файл себе.


(!) гораздо удобнее создать алерт из работающего дашборда.
    Для этого нужно открыть на редактирование панель на любом дашборде и,
    перейдя на вкладку Alert, начать создавать новый нажав на
    `Create alert rule from this panel`

В открывшемся окне есть несколько шагов, с помощью которых можно создать алерт:
  Шаг 1. Rule name — указываем имя правила.
  Шаг 2. Rule type — используем Grafana managed alert
  Шаг 3. В поле Folder укажите имя новой директории для этого правила.
         Пусть вас не смущает сообщение «No options found», просто начните набирать имя директории
  Шаг 4. Create a query to be alerted on
         — показаны как существующие PromQL-запросы,
           так и созданные правила для проверки, помеченные как __expr__,
           в которых можно настроить триггер срабатывания Alert'а.

Например:
...

Здесь определяется условие, при котором последние показатели запроса "А" (оно же среднее время ответа сервиса в секундах) больше значения 3.
  Шаг 5. Define alert conditions
    = время, в течение которого должно выполняться наше условие,
    а также что нужно делать, если условие по каким-то причинам не вернуло данных (Configure no data and error handling).

  Шаг 6. Add details for your alert
    = задаём описание для того, кто получит оповещение.
      Можно указать уровень проблемы или ссылку на документ с инструкцией по решению проблемы.

На этом мастер по созданию алерта завершает свою работу, и мы можем посмотреть, что получилось:


(?) Как же связать наш новенький алерт и отправку в Telegram?

[вкладка Alerting] -> Notification policies:

"""
*Root policy - default for all alerts*
All alerts will go to the default contact point, unless you set additional matchers in the specific routing area.

Contact point       grafana-default-email
...
"""

(!) можно поменять этот дефолтный способ (email) на Telegram,
и тогда все оповещения будут уходить в наш командный channel!


(?) а если нужно для разных алертов использовать разные каналы?

Строится всё на label:
  1) добавляешь к нашим алертам дополнительный label, например messenger = telegram,
  2) затем переходим на вкладку Alerting -> Notification policies,
     -> где добавляем новый policy, чтобы направить все оповещения с этих алертов на наш Telegram


################
# Alertmanager #
################

  Основными компонентами Prometheus являются:
    * сам сервер, выполняющий периодический опрос экспортёров
    * база данных временных рядов (англ. time series database, TSDB) — хранилище метрик
    * алертменеджер для отправки оповещений

Prometheus отдаёт на аутсорс работу с оповещениями,
!НО!  ПРАВИЛА-ТРИГГЕРЫ определяет САМ.

В момент срабатывания триггера Prometheus формирует одно || несколько сообщений
со всей необходимой информацией:
  * происхождение,
  * важность
  * и время появления алерта

и отправляет их самым обычным HTTP Post-запросом на обработку -(в)-> Alertmanager.

ПУТЬ К ФАЙЛАМ КОНФИГУРАЦИИ таких ПРАВИЛ-ТРИГГЕРОВ можно указать
  -> в глобальном конфиге Prometheus в параметре  rule_files.

[!] Для нашего Prometheus:
  такие правила указываются в ConfigMap с именем prometheus-rules:

[!!!] А сами правила описаны в <chart>/rules/sausage-store.yaml

-->
kubectl get cm/prometheus-rules -o yaml
<--

#> Перезапустите Pod Prometheus после изменения файла конфигурации правил.

# Вот так может выглядеть правило для оповещения о недоступности пода бэкенда сосисочной:

--<rule>--
---
groups:
- name: Sausage store rules
  rules:
  - alert: InstanceDown
    expr: up{app="backend"} == 0
    for: 1m
    labels:
      severity: critical
      project: "sausage_store"
      component: "Backend"


  alert — имя алерта.
  expr — условие на языке PromQL, опирающееся на метрики в Prometheus. В этом примере используется метрика up, которую Prometheus добавляет к каждому инстансу.
  for — время, в течение которого должно срабатывать условие.
  labels.severity — метки, которые можно использовать для указания уровня проблемы и другой полезной информации.


А для настройки интеграции с Alertmanager в Prometheus-конфиге
(у нас это prometheus.yml в ConfigMap с именем prometheus-conf)
([helm: <chart>/templates/configmap.yaml])
используется директива alerting:
---
alerting:
  alertmanagers:
  - scheme: http
    static_configs:
    - targets:
      - "alertmanager:9093"


Тут мы отправляем алерты по http на адрес alertmanager по 9093 порту.
В рамках одного неймспейса в Kubernetes работает разрешение имен по имени работающего сервиса
(тот самый kind: Service в терминологии объектов Kubernetes).

Сейчас мы установим такой сервис, а пока перезапустите Pod Prometheus после изменения файла конфигурации в ConfigMap.

/*
  Для установки Alertmanager мы возьмём Helm-чарт из того же репозитория
  Ставим его так же, как до этого Grafana и Prometheus — меняем переменные и запускаем команду установки
*/

В уроках про GitLab вы уже настраивали нотификации в Telegram об успешной сборке.

(!) Возьмите данные оттуда и укажите В ФАЙЛЕ ПЕРЕМЕННЫХ ЧАРТА:
alertmanager/values.yaml:

  bot_token — секретный токен бота,
  chat_id — идентификатор канала вашей когорты.

--[]--
$ cd monitoring-tools
$ helm upgrade --atomic --install alertmanager alertmanager

Helm-чарт установит вот такой конфиг для Alertmanager в ConfigMap с именем «alertmanager»:

--<default-configmap.yaml>--
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager
  namespace: {{ .Release.Namespace }}
  labels:
    app: alertmanager
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 1m
      slack_api_url: '{{ .Values.slack_hook_url }}'
    route:
       group_wait: 10s
       group_interval: 30s
       repeat_interval: 30m
       receiver: "slack"
       routes:
         - receiver: "slack"
           group_wait: 10s
           match_re:
             severity: critical|warning
           continue: true
    receivers:
     - name: "slack"
       slack_configs:
         - send_resolved: true
           channel: '{{ .Values.slack_channel }}'
           text: "{{ "{{" }} range .Alerts {{ "}}" }}{{ "{{" }} .Annotations.summary {{ "}}" }}\n{{ "{{" }} .Annotations.description }}\n{{ "{{" }} end {{ "}}" }}"


--<configmap-telegram.yaml>--
# alertmanager.yaml
---
global:
  resolve_timeout: 1m

route:
  group_wait: 10s
  group_interval: 30s
  repeat_interval: 30m
  receiver: "telegram"
  routes:
    - receiver: "telegram"
      group_wait: 10s
      match_re:
        severity: critical|warning
      continue: true

receivers:
 - name: "telegram"
   telegram_configs:
   - send_resolved: true
     bot_token: 'IAMSECRETTOKENFORTELEGRAMVERYLONGLONGSTRING'
     chat_id: -000000000000
     message: "{{ range .Alerts }}{{ .Annotations.description }}\n{{ end }}"


  ● global — перечень конфигураций, доступных для всех систем алертинга;
  ● route.receiver — основной способ алертинга;
  ● routes
      = список правил, сопоставляющих алерты с получателями.
        Для различных алертов, например, по уровню важности, (!) можно определить различных получателей;
  ● receivers.name
      = имя получателя (англ. «receiver»), это могут быть email-сообщения, Slack и другие мессенджеры;
  ● receivers.telegram_configs
      = конфигурация для Telegram, в нашем случае это id канала, bot_token и шаблон текста алерта.

#-> Configuration: https://prometheus.io/docs/alerting/latest/configuration/

########################
# Ключевые мысли урока #
########################

  Алерты нужны для своевременного оповещения о проблемах
  На такие алерты реагирует дежурный и стоит аккуратно подходить к процессу конфигурации оповещений
  Для простых метрик, таких как свободное место на диске, алерты можно настроить по абсолютному значению. Перцентиль подойдёт для оценки большого количества измерений.
  В Grafana есть собственная система оповещений с гибкой конфигурацией
  Prometheus отдаёт на аутсорс работу с оповещениями отдельному процессу — Alertmanager

######################
# Полезные материалы #
######################

Being On-Call:
  https://response.pagerduty.com/oncall/being_oncall/

Postmortem Culture: Learning from Failure
  https://sre.google/sre-book/postmortem-culture/

Create Grafana managed alert
  https://grafana.com/docs/grafana/latest/alerting/alerting-rules/create-grafana-managed-rule/

Alertmanager
  https://prometheus.io/docs/alerting/latest/alertmanager/